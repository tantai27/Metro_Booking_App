{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#cell 1\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 2\n# ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\narchive_dir = \"/kaggle/input/skin-cancer-mnist-ham10000\"\npart1_dir = os.path.join(archive_dir, \"HAM10000_images_part_1\")\npart2_dir = os.path.join(archive_dir, \"HAM10000_images_part_2\")\nmetadata_file = os.path.join(archive_dir, \"HAM10000_metadata.csv\")\nmerged_dir = \"/kaggle/working/HAM10000_sorted\"\nos.makedirs(merged_dir, exist_ok=True)\n\n# ƒê·ªçc metadata v√† s·∫Øp x·∫øp ·∫£nh\ndf = pd.read_csv(metadata_file)\nfor _, row in df.iterrows():\n    image_id, disease_class = row[\"image_id\"], row[\"dx\"]\n    class_dir = os.path.join(merged_dir, disease_class)\n    os.makedirs(class_dir, exist_ok=True)\n    filename = image_id + \".jpg\"\n    src_path = os.path.join(part1_dir, filename) if os.path.exists(os.path.join(part1_dir, filename)) else os.path.join(part2_dir, filename)\n    if os.path.exists(src_path):\n        shutil.copy(src_path, os.path.join(class_dir, filename))\n\nprint(f\"‚úÖ ƒê√£ g·ªôp v√† s·∫Øp x·∫øp ·∫£nh v√†o th∆∞ m·ª•c: {merged_dir}\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 3\nfrom torch.quantization import QuantStub, DeQuantStub, fuse_modules\n\ndata_dir = merged_dir\noutput_dir = \"/kaggle/working/HAM10000_split\"\nfor split in [\"train\", \"valid\", \"test\"]:\n    os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n\ntrain_ratio, valid_ratio, test_ratio = 0.8, 0.1, 0.1\nassert train_ratio + valid_ratio + test_ratio == 1.0, \"T·ªïng t·ª∑ l·ªá ph·∫£i b·∫±ng 1\"\n\nfor class_name in os.listdir(data_dir):\n    class_path = os.path.join(data_dir, class_name)\n    if not os.path.isdir(class_path): continue\n\n    for split in [\"train\", \"valid\", \"test\"]:\n        os.makedirs(os.path.join(output_dir, split, class_name), exist_ok=True)\n\n    images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png'))]\n    train_images, temp_images = train_test_split(images, train_size=train_ratio, random_state=42)\n    valid_images, test_images = train_test_split(temp_images, test_size=0.5, random_state=42)\n\n    def copy_images(img_list, src_dir, dest_dir):\n        for img in img_list:\n            shutil.copy2(os.path.join(src_dir, img), os.path.join(dest_dir, img))\n\n    copy_images(train_images, class_path, os.path.join(output_dir, \"train\", class_name))\n    copy_images(valid_images, class_path, os.path.join(output_dir, \"valid\", class_name))\n    copy_images(test_images, class_path, os.path.join(output_dir, \"test\", class_name))\n\n    print(f\"‚úÖ {class_name}: {len(train_images)} train, {len(valid_images)} valid, {len(test_images)} test\")\n\nprint(\"\\nüéØ Ho√†n th√†nh! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia v√†o '/kaggle/working/HAM10000_split'.\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 4\nimport torch.ao.quantization as tq\n\nclass CustomEfficientNet(nn.Module):\n    def __init__(self, num_classes):\n        super(CustomEfficientNet, self).__init__()\n        self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n\n        # Th√™m l∆∞·ª£ng t·ª≠ h√≥a\n        self.quant = tq.QuantStub()\n        self.dequant = tq.DeQuantStub()\n\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.6),\n            nn.Linear(1280, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.quant(x)  # L∆∞·ª£ng t·ª≠ h√≥a ƒë·∫ßu v√†o\n        x = self.model(x)\n        x = self.dequant(x)  # Gi·∫£i l∆∞·ª£ng t·ª≠ h√≥a ƒë·∫ßu ra\n        return x\n\n    def fuse_model(self):\n        for module_name, module in self.model.named_children():\n            if isinstance(module, nn.Sequential):\n                for block_name, block in module.named_children():\n                    if isinstance(block, nn.Sequential):\n                        submodules = list(block.children())\n\n                        # Ki·ªÉm tra v√† fuse Conv2d + BatchNorm2d + ReLU n·∫øu c√≥\n                        for i in range(len(submodules) - 2):\n                            if (\n                                isinstance(submodules[i], nn.Conv2d)\n                                and isinstance(submodules[i + 1], nn.BatchNorm2d)\n                                and isinstance(submodules[i + 2], nn.ReLU)\n                            ):\n                                tq.fuse_modules(block, [str(i), str(i + 1), str(i + 2)], inplace=True)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell 5\nimport torch.ao.quantization as tq\n\ndef fuse_model(model):\n    \"\"\" H·ª£p nh·∫•t (fuse) c√°c l·ªõp Conv + BatchNorm + ReLU tr∆∞·ªõc khi QAT \"\"\"\n    for module_name, module in model.named_children():\n        if isinstance(module, torch.nn.Sequential):\n            submodules = list(module.children())\n            if (\n                len(submodules) >= 3\n                and isinstance(submodules[0], torch.nn.Conv2d)\n                and isinstance(submodules[1], torch.nn.BatchNorm2d)\n                and isinstance(submodules[2], torch.nn.ReLU)\n            ):\n                tq.fuse_modules(module, ['0', '1', '2'], inplace=True)\n        fuse_model(module)\n\ndef evaluate(model, device, loader, loss_fn, mode=\"Validation\"):\n    model.eval()\n    total_loss, correct, total = 0, 0, 0\n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            total_loss += loss_fn(output, target).item()\n            correct += output.argmax(dim=1).eq(target).sum().item()\n            total += target.size(0)\n\n    avg_loss = total_loss / len(loader)\n    accuracy = 100. * correct / total\n    print(f\"{mode}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n    return avg_loss, accuracy\n\ndef train(args, model, device, train_loader, valid_loader, optimizer, scheduler, early_stop):\n    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n    train_losses, valid_losses = [], []\n    best_valid_loss = float(\"inf\")\n\n    # üîπ B·∫≠t ch·∫ø ƒë·ªô QAT tr∆∞·ªõc khi train (c·∫ßn ki·ªÉm tra n·∫øu ch∆∞a chu·∫©n b·ªã)\n    if not hasattr(model, 'qconfig') or model.qconfig is None:\n        model.qconfig = tq.get_default_qat_qconfig('qnnpack')  # Ho·∫∑c 'fbgemm' n·∫øu b·∫°n s·ª≠ d·ª•ng GPU\n        fuse_model(model)  # H·ª£p nh·∫•t c√°c l·ªõp tr∆∞·ªõc khi QAT\n        tq.prepare_qat(model, inplace=True)\n        print(\"‚úÖ QAT Enabled: Model prepared for Quantization Aware Training\")\n\n    for epoch in range(1, args.epochs + 1):\n        print(f\"\\nEpoch {epoch}/{args.epochs}\")\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n\n        for data, target in train_loader:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            correct += output.argmax(dim=1).eq(target).sum().item()\n            total += target.size(0)\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = 100. * correct / total  # T√≠nh accuracy tr√™n t·∫≠p train\n        train_losses.append(avg_train_loss)\n\n        valid_loss, valid_accuracy = evaluate(model, device, valid_loader, loss_fn)\n        valid_losses.append(valid_loss)\n\n        # ‚úÖ In ƒë·∫ßy ƒë·ªß th√¥ng tin\n        print(f\"Train: Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n\n        scheduler.step(valid_loss)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), \"best_model.pt\")\n            print(\"Model improved and saved!\")\n            early_stop.counter = 0\n        else:\n            early_stop.counter += 1\n            if early_stop.counter >= early_stop.patience:\n                print(\"Early stopping triggered!\")\n                break\n\n    return train_losses, valid_losses\n\n\ndef test(model, device, test_loader):\n    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n    print(\"\\nFinal Evaluation on Test Set:\")\n    evaluate(model, device, test_loader, loss_fn, mode=\"Test\")\n\nclass EarlyStopping:\n    def __init__(self, patience=5):\n        self.patience = patience\n        self.counter = 0\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Args:\n    batch_size = 32\n    test_batch_size = 32\n    epochs = 40\n    lr = 0.0001\n    patience = 7\n\nargs = Args()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_dir, valid_dir, test_dir = [os.path.join(output_dir, d) for d in [\"train\", \"valid\", \"test\"]]\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\nvalid_dataset = datasets.ImageFolder(root=valid_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.test_batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n\nnum_classes = len(train_dataset.classes)\nmodel = CustomEfficientNet(num_classes).to(device)\noptimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=5e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\nearly_stop = EarlyStopping(patience=args.patience)\n\n# üî• Train m√¥ h√¨nh\ntrain(args, model, device, train_loader, valid_loader, optimizer, scheduler, early_stop)\n\n# üîÑ Load l·∫°i m√¥ h√¨nh t·ªët nh·∫•t\nmodel.load_state_dict(torch.load(\"best_model.pt\", weights_only=True))\nmodel.to(\"cpu\")  # Chuy·ªÉn v·ªÅ CPU tr∆∞·ªõc khi l∆∞·ª£ng t·ª≠ h√≥a\nmodel.eval()\n\n# üõ†Ô∏è Fuse model tr∆∞·ªõc khi l∆∞·ª£ng t·ª≠ h√≥a ƒë·ªÉ tr√°nh l·ªói fuser method\nmodel.fuse_model()  # ‚úÖ G·ªçi ph∆∞∆°ng th·ª©c fuse_model() ƒë√∫ng c√°ch\n\n# üî• Ch·ªâ l∆∞·ª£ng t·ª≠ h√≥a n·∫øu m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã cho QAT tr∆∞·ªõc ƒë√≥\nif hasattr(model, 'qconfig') and model.qconfig is not None:\n    torch.ao.quantization.convert(model, inplace=True)\n    print(\"‚úÖ Model has been quantized successfully!\")\n    torch.save(model.state_dict(), \"best_model_quantized.pt\")  # L∆∞u m√¥ h√¨nh l∆∞·ª£ng t·ª≠ h√≥a\nelse:\n    print(\"‚ö†Ô∏è Warning: Model was not prepared for QAT, skipping quantization!\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}